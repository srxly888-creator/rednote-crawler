[English](crawler_development_wiki.md) | 简体中文

# AI 结对编程指南：零基础驾驭小红书爬虫

> **写给小白的话**：
> 时代变了。你不需要成为一个精通 Python 的程序员才能使用这个工具。
> 你只需要会安装一个**AI 编辑器 (如 Python Antigravity, Cursor)**，然后像“产品经理”一样给 AI 下达指令，剩下的交给它。

---

## 🚀 核心理念：你是 PM，AI 是工程师

在这个项目中，你的角色不是“写代码的人”，而是 **产品经理 (Product Manager)**。
你的工作是：**提出需求、验收结果、指出问题**。
脏活累活（安装环境、修复报错、修改逻辑）全部交给 IDE 里的 AI。

---

## 🛠️ 第一步：准备你的“AI 员工”

忘记复杂的 Python 安装教程，我们通过现代 IDE 让 AI 帮我们搞定一切。

### 推荐工具
1.  **Google Antigravity** (本项目原生支持)
2.  **Cursor** / **VS Code + Copilot**

### 安装步骤
1.  下载并安装上述任意一个 IDE。
2.  用 IDE 打开本项目文件夹 (`xhs_crawler_system`)。
3.  **关键一步**：打开右侧的 AI 对话框 (Chat)。

---

## ⚡️ 第二步：一键环境配置

不需要你去终端敲命令，直接在 AI Chat 对话框里输入指令。

**👉 复制这段话发给 AI：**

> “我是小白，刚下载了这个项目。请帮我检查并配置好 Python 运行环境，安装所有依赖（requirements.txt）。如果需要创建虚拟环境，请自动帮我执行。”

**AI 会做什么？**
- 它会自动检测你的电脑有没有 Python。
- 它会自动在终端执行 `bash setup_env.sh` 或 `pip install ...`。
- 如果遇到权限报错，它会自己尝试修复或提示你输入密码。

**看到 `Install successfully` 或 `依赖安装完成` 提示后，即可进入下一步。**

---

## 🎮 第三步：运行爬虫

现在环境好了，我们来跑一个最简单的采集任务。

**👉 复制这段话发给 AI：**

> “我想试运行一下 `simple_scrape.py` 这个脚本，请帮我运行它。如果有需要登录的地方，请告诉我。”

**接下来会发生什么？**
1.  **浏览器弹窗**：AI 会启动一个 Chrome 浏览器窗口。
2.  **手动辅助**：如果控制台提示“需要登录”，请你在那个**自动弹出的浏览器窗口里**扫码登录你的小红书账号。
3.  **接力**：登录完后，告诉 AI：“**我登录好了，继续**”（或者在终端按回车）。
4.  **自动采集**：你会看到浏览器自动搜索“遮瑕”、点击笔记、翻阅评论。

---

## 🎨 第四步：定制你的需求 (Magic Time)

这是最神奇的地方。你不需要改代码，只需要通过对话修改爬虫的行为。

### 场景 A：我想搜别的词
**🧔 你 (PM)**：“帮我把 `simple_scrape.py` 里的搜索关键词改成 '男士猫粮'，可以吗？”
**🤖 AI (Dev)**：会自动定位到代码中的 `keyword = "遮瑕"` 并将其修改为 `"男士猫粮"`。

### 场景 B：我想多抓点评论
**🧔 你 (PM)**：“现在的脚本评论抓得太少了，能不能让它多往下翻几页？比如翻个 20 次？”
**🤖 AI (Dev)**：会修改代码中的循环次数，比如将 `range(10)` 改为 `range(20)`。

### 场景 C：报错了怎么办？
**🧔 你 (PM)**：（直接把报错信息截图或复制给 AI）“程序报错了，帮我看看怎么修？”
**🤖 AI (Dev)**：会自动分析错误栈，如果是选择器失效了，它甚至会建议你更新代码。

---
---

# 进阶：技术架构与原理 (Under the Hood)

*(如果你对它是如何工作的感兴趣，可以阅读本节)*

## 1. 项目概述

本项目是一个针对小红书平台的自动化数据采集系统。核心目标是采集特定关键词（如“美妆”、“护肤”）下的笔记、评论数据，并为后续的数据分析提供结构化支持。

### 技术栈

-   **Language**: Python 3.10+
-   **Browser Automation**: [DrissionPage](https://g1879.gitee.io/drissionpage/) (ChromiumPage mode)
-   **Database**: SQLite + [SQLModel](https://sqlmodel.tiangolo.com/) (ORM)

## 2. 核心架构设计

系统主要由以下模块组成：

1.  **Crawler Core (`crawler/xhs_crawler.py`)**: 负责控制浏览器行为，执行搜索、翻页、详情抓取、Cookie 管理等。
2.  **Data Models (`database/models.py`)**: 定义 `Note` (笔记) 和 `Comment` (评论) 的数据库结构。
3.  **Manager (`crawler/crawler_manager.py`)**: (逻辑层) 调度爬虫任务，处理异常重试。

## 3. 我们是如何利用 AI 开发这个项目的？

本项目本身就是 **AI-Native** 开发流程的成果。

1.  **Agentic Mode (代理模式)**:
    -   AI 不仅仅回答问题，而是作为“独立开发者”接管任务。它主动创建 `task.md` 规划进度，维护 `implementation_plan.md` 设计方案。
    -   例如在重构爬虫架构时，AI 主动提出了架构方案并自动执行了文件移动。

2.  **调试闭环 (Debug Loop)**:
    -   遇到 CSS 选择器失效时，AI 会编写一次性的 `debug_xhs.py` 脚本，抓取当前页面 HTML Dump，分析后修正选择器，而不是让人类反复手动测试。

3.  **文档即代码 (Docs as Code)**:
    -   所有的文档（包括本文）都是 AI 根据代码变更自动起草维护的。
